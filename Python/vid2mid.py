"""

===============================================
一种基于帧图像识别的预览视频扒谱方式.
完全由AI生成，使用时请按需修改 (主要修改音符位置表及高亮颜色).
测试功能所使用的视频: https://www.bilibili.com/video/BV1gps8zjEjo, 可完整扒谱.
限制: 当同一个键中途没有一帧是完全不亮的情况下，会误判为持续按下.

Using image analysis to convert a piano roll video (highlighted keys) into a MIDI file.
Completely generated by AI, update parameters as needed (especially the key map and color scheme).
Video used to test: https://www.youtube.com/watch?v=bTEq7xuv46s, able to fully transcribe.
Limitations: When the same key are consecutively highlighted without any frame of "off" state, it will misinterpret as sustained press.
===============================================

Tags: python 3.14, opencv, mido, midi, video & image processing, music, 扒谱, 音乐, 视频 图像 处理

"""

from __future__ import annotations

import os
from dataclasses import dataclass
from typing import List, Tuple, Dict

import cv2
import numpy as np
import mido


# =========================
# CONFIGURATION
# =========================

INPUT_VIDEO_PATH = r"./video.mp4"
OUTPUT_MIDI_PATH = r"./output.mid"

FPS = 60.0
BPM = 112.0

# Crop band: y in [960, 970)
CROP_Y0 = 960
CROP_Y1 = 970

# MIDI
TICKS_PER_BEAT = 480
VELOCITY = 100

# Timing alignment / snapping
START_AT_BEATS = 2.25          # shift earliest note to land here
QUANTIZE_DIV = 4               # grid = TPQ / QUANTIZE_DIV; 4 -> 1/16 note (quarter-beat grid)

# Robust detection knobs
EXPECTED_WIDTH = 1920          # your x-map is for 1920px width
KEY_X_MARGIN = 1               # keep small; black keys are narrow
MIN_V = 155                    # brightness gate (HSV V)
MIN_S = 45                     # saturation gate (HSV S)

DIST_THRESH = 110              # RGB distance to ref colors to count as "highlight"
MIN_HIT_PIXELS = 10            # absolute count threshold (total)
MIN_HIT_RATIO = 0.015          # ratio threshold (total)

# --- anti-bleed (adjacent key false positives) ---
CORE_MARGIN = 2                # pixels trimmed from LEFT/RIGHT within each key ROI to define "core"
MIN_CORE_PIXELS = 6            # absolute highlighted pixels required in the core
MIN_CORE_FRACTION = 0.30       # core_pixels / total_pixels must be at least this

# Debounce / gap tolerance
END_GAP_FRAMES = 1


# Reference highlight colors (RGB)
REF_ORANGE = np.array([247, 204, 101], dtype=np.int32)  # #F7CC65
REF_YELLOW = np.array([253, 246, 122], dtype=np.int32)  # #FDF67A


# =========================
# KEY MAP: (name, x0, x1, midi_note)  x ranges are [x0, x1)
# =========================
KEY_RANGES: List[Tuple[str, int, int, int]] = [
    ("A0",5,26,21),("A#0",36,48,22),("B0",58,70,23),("C1",80,93,24),("C#1",103,115,25),
    ("D1",125,136,26),("D#1",146,158,27),("E1",168,181,28),("F1",191,201,29),("F#1",211,223,30),
    ("G1",233,243,31),("G#1",253,265,32),("A1",275,285,33),("A#1",295,307,34),("B1",317,330,35),
    ("C2",340,351,36),("C#2",361,373,37),("D2",383,395,38),("D#2",405,416,39),("E2",426,439,40),
    ("F2",449,460,41),("F#2",470,483,42),("G2",493,504,43),("G#2",514,526,44),("A2",536,548,45),
    ("A#2",558,569,46),("B2",579,592,47),("C3",602,614,48),("C#3",624,635,49),("D3",645,657,50),
    ("D#3",667,679,51),("E3",689,701,52),("F3",711,721,53),("F#3",731,744,54),("G3",754,765,55),
    ("G#3",775,787,56),("A3",797,809,57),("A#3",819,830,58),("B3",840,852,59),("C4",862,874,60),
    ("C#4",884,896,61),("D4",906,917,62),("D#4",927,939,63),("E4",949,962,64),("F4",972,983,65),
    ("F#4",993,1005,66),("G4",1015,1026,67),("G#4",1036,1048,68),("A4",1058,1070,69),("A#4",1080,1080,70),
    ("B4",1090,1102,71),("C5",1112,1126,72),("C#5",1136,1148,73),("D5",1158,1170,74),("D#5",1180,1191,75),
    ("E5",1201,1213,76),("F5",1223,1235,77),("F#5",1245,1257,78),("G5",1267,1278,79),("G#5",1288,1301,80),
    ("A5",1311,1322,81),("A#5",1332,1343,82),("B5",1353,1365,83),("C6",1375,1387,84),("C#6",1397,1409,85),
    ("D6",1419,1431,86),("D#6",1441,1452,87),("E6",1462,1475,88),("F6",1485,1496,89),("F#6",1506,1519,90),
    ("G6",1529,1536,91),("G#6",1546,1558,92),("A6",1568,1579,93),("A#6",1589,1601,94),("B6",1611,1620,95),
    ("C7",1630,1643,96),("C#7",1653,1665,97),("D7",1675,1686,98),("D#7",1696,1708,99),("E7",1718,1730,100),
    ("F7",1740,1751,101),("F#7",1761,1774,102),("G7",1784,1793,103),("G#7",1803,1815,104),("A7",1825,1835,105),
    ("A#7",1845,1857,106),("B7",1867,1877,107),("C8",1887,1915,108),
]


@dataclass
class NoteEvent:
    track_id: int   # 1 = yellow, 2 = orange
    midi_note: int
    start_frame: int
    end_frame: int  # exclusive


def frames_to_ticks(frame_idx: int, fps: float, bpm: float, tpq: int) -> int:
    return int(round((frame_idx / fps) * (bpm / 60.0) * tpq))


def quantize_tick(t: int, grid: int) -> int:
    return int(round(t / grid) * grid)


def detect_track_for_key(roi_bgr: np.ndarray) -> int:
    """
    Returns 0 (not pressed), 1 (yellow), 2 (orange).

    Anti-bleed logic:
      - compute highlight mask (HSV gate + RGB distance)
      - require enough highlight in the *core* region of the key (trim edges)
      - classify track using only core pixels (avoids neighbor glow influencing vote)
    """
    if roi_bgr.size == 0:
        return 0

    hsv = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2HSV)
    S = hsv[:, :, 1]
    V = hsv[:, :, 2]
    gate = (S >= MIN_S) & (V >= MIN_V)

    roi_rgb = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2RGB).astype(np.int32)

    d_or = np.sqrt(((roi_rgb - REF_ORANGE) ** 2).sum(axis=2))
    d_ye = np.sqrt(((roi_rgb - REF_YELLOW) ** 2).sum(axis=2))
    d_min = np.minimum(d_or, d_ye)

    mask = gate & (d_min <= DIST_THRESH)

    hit_total = int(mask.sum())
    if hit_total < MIN_HIT_PIXELS and (hit_total / mask.size) < MIN_HIT_RATIO:
        return 0

    # Core region (trim left/right), fallback if ROI is too narrow
    h, w = mask.shape
    cm = CORE_MARGIN if w > (2 * CORE_MARGIN + 2) else 0
    if cm > 0:
        core = mask[:, cm:w - cm]
        d_or_core = d_or[:, cm:w - cm]
        d_ye_core = d_ye[:, cm:w - cm]
    else:
        core = mask
        d_or_core = d_or
        d_ye_core = d_ye

    hit_core = int(core.sum())
    core_fraction = (hit_core / hit_total) if hit_total > 0 else 0.0

    # Require meaningful core presence to avoid neighbor spill triggering
    if hit_core < MIN_CORE_PIXELS or core_fraction < MIN_CORE_FRACTION:
        return 0

    # Vote on core pixels only
    or_votes = int(((d_or_core < d_ye_core) & core).sum())
    ye_votes = int(((d_ye_core <= d_or_core) & core).sum())

    return 2 if or_votes > ye_votes else 1


def build_midi(note_events: List[NoteEvent], out_path: str) -> None:
    mid = mido.MidiFile(ticks_per_beat=TICKS_PER_BEAT)
    tempo_us = mido.bpm2tempo(BPM)

    meta = mido.MidiTrack()
    meta.append(mido.MetaMessage("set_tempo", tempo=tempo_us, time=0))
    meta.append(mido.MetaMessage("time_signature", numerator=4, denominator=4, time=0))
    mid.tracks.append(meta)

    track_y = mido.MidiTrack()
    track_y.append(mido.MetaMessage("track_name", name="Yellow", time=0))
    mid.tracks.append(track_y)

    track_o = mido.MidiTrack()
    track_o.append(mido.MetaMessage("track_name", name="Orange", time=0))
    mid.tracks.append(track_o)

    msgs_by_track: Dict[int, List[Tuple[int, mido.Message]]] = {1: [], 2: []}
    grid = max(1, int(TICKS_PER_BEAT / QUANTIZE_DIV))

    # Shift so earliest note starts at START_AT_BEATS (after quantization)
    if note_events:
        earliest_frame = min(ev.start_frame for ev in note_events)
        earliest_tick = frames_to_ticks(earliest_frame, FPS, BPM, TICKS_PER_BEAT)
    else:
        earliest_tick = 0

    desired_start_tick = int(round(START_AT_BEATS * TICKS_PER_BEAT))
    shift = desired_start_tick - quantize_tick(earliest_tick, grid)

    for ev in note_events:
        start_tick = frames_to_ticks(ev.start_frame, FPS, BPM, TICKS_PER_BEAT) + shift
        end_tick = frames_to_ticks(ev.end_frame, FPS, BPM, TICKS_PER_BEAT) + shift

        start_tick_q = max(0, quantize_tick(start_tick, grid))
        end_tick_q = max(0, quantize_tick(end_tick, grid))
        if end_tick_q <= start_tick_q:
            end_tick_q = start_tick_q + 1

        ch = 0 if ev.track_id == 1 else 1
        on = mido.Message("note_on", note=ev.midi_note, velocity=VELOCITY, time=0, channel=ch)
        off = mido.Message("note_off", note=ev.midi_note, velocity=0, time=0, channel=ch)

        msgs_by_track[ev.track_id].append((start_tick_q, on))
        msgs_by_track[ev.track_id].append((end_tick_q, off))

    def write_track(track: mido.MidiTrack, abs_msgs: List[Tuple[int, mido.Message]]) -> None:
        abs_msgs.sort(key=lambda x: x[0])
        prev = 0
        for t, msg in abs_msgs:
            delta = t - prev
            prev = t
            msg.time = max(0, int(delta))
            track.append(msg)

    write_track(track_y, msgs_by_track[1])
    write_track(track_o, msgs_by_track[2])

    mid.save(out_path)


def video_to_note_events(video_path: str) -> List[NoteEvent]:
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Could not open video: {video_path}")

    events: List[NoteEvent] = []

    active_track = np.zeros(len(KEY_RANGES), dtype=np.int16)   # 0/1/2
    start_frame = np.full(len(KEY_RANGES), -1, dtype=np.int32)
    miss_count = np.zeros(len(KEY_RANGES), dtype=np.int16)

    frame_idx = 0

    while True:
        ok, frame = cap.read()
        if not ok:
            break

        h, w = frame.shape[:2]
        scale = (w / EXPECTED_WIDTH) if EXPECTED_WIDTH > 0 else 1.0

        y0 = max(0, min(CROP_Y0, h))
        y1 = max(0, min(CROP_Y1, h))
        if y1 <= y0:
            raise RuntimeError(f"Crop invalid for height={h}: [{CROP_Y0},{CROP_Y1})")

        band = frame[y0:y1, :, :]  # BGR

        curr_track = np.zeros(len(KEY_RANGES), dtype=np.int16)

        for i, (_name, x0, x1, _mnote) in enumerate(KEY_RANGES):
            xs = int(round(x0 * scale)) + KEY_X_MARGIN
            xe = int(round(x1 * scale)) - KEY_X_MARGIN
            xs = max(0, min(xs, w))
            xe = max(0, min(xe, w))
            if xe <= xs:
                curr_track[i] = 0
                continue

            roi = band[:, xs:xe, :]
            curr_track[i] = detect_track_for_key(roi)

        # Update state with gap tolerance
        for i, (_name, _x0, _x1, midi_note) in enumerate(KEY_RANGES):
            at = int(active_track[i])
            ct = int(curr_track[i])

            if at == 0:
                if ct != 0:
                    active_track[i] = ct
                    start_frame[i] = frame_idx
                    miss_count[i] = 0
            else:
                if ct == 0:
                    miss_count[i] += 1
                    if miss_count[i] > END_GAP_FRAMES:
                        events.append(NoteEvent(
                            track_id=at,
                            midi_note=midi_note,
                            start_frame=int(start_frame[i]),
                            end_frame=frame_idx,
                        ))
                        active_track[i] = 0
                        start_frame[i] = -1
                        miss_count[i] = 0
                else:
                    miss_count[i] = 0
                    if ct != at:
                        events.append(NoteEvent(
                            track_id=at,
                            midi_note=midi_note,
                            start_frame=int(start_frame[i]),
                            end_frame=frame_idx,
                        ))
                        active_track[i] = ct
                        start_frame[i] = frame_idx

        frame_idx += 1

    # Flush active notes
    for i, (_name, _x0, _x1, midi_note) in enumerate(KEY_RANGES):
        at = int(active_track[i])
        if at != 0 and start_frame[i] >= 0:
            events.append(NoteEvent(
                track_id=at,
                midi_note=midi_note,
                start_frame=int(start_frame[i]),
                end_frame=frame_idx,
            ))

    cap.release()
    return events


def main() -> None:
    out_dir = os.path.dirname(OUTPUT_MIDI_PATH)
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)

    note_events = video_to_note_events(INPUT_VIDEO_PATH)
    build_midi(note_events, OUTPUT_MIDI_PATH)

    print(f"Done. Wrote MIDI: {OUTPUT_MIDI_PATH}")
    print(f"Detected note events: {len(note_events)}")


if __name__ == "__main__":
    main()
